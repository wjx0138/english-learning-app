#!/usr/bin/env python3
"""
å®Œæ•´è¯åº“ç³»ç»Ÿç”Ÿæˆå™¨
ç”Ÿæˆå¤šçº§åˆ«ã€å¤šç±»åˆ«çš„å®Œæ•´è¯åº“ä½“ç³»
"""

import json
import os
from typing import List, Dict

# å®Œæ•´è¯æ±‡åº“ - åŒ…å«CET4/6æ ¸å¿ƒè¯æ±‡
COMPREHENSIVE_VOCABULARY_DB = {
    # ä½¿ç”¨ä¹‹å‰ç”Ÿæˆçš„è¯æ±‡åº“
}

def load_existing_vocabs() -> Dict:
    """åŠ è½½å·²æœ‰çš„è¯æ±‡åº“"""
    vocab_db = {}

    vocab_files = [
        "../assets/vocabularies/cet4_complete.json",
        "../assets/vocabularies/cet6_complete.json",
        "../assets/vocabularies/toefl_complete.json",
        "../assets/vocabularies/ielts_complete.json",
    ]

    for file_path in vocab_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    vocab_list = json.load(f)
                    for entry in vocab_list:
                        word = entry['word']
                        if word not in vocab_db:
                            vocab_db[word] = entry
            except Exception as e:
                print(f"Warning: Could not load {file_path}: {e}")

    return vocab_db

def create_categorized_vocabulary():
    """åˆ›å»ºåˆ†ç±»è¯åº“"""
    print("\nğŸ“š åˆ›å»ºåˆ†ç±»è¯åº“ç³»ç»Ÿ...")

    # è¯æ±‡åˆ†ç±»
    categories = {
        "daily_life": {
            "name": "æ—¥å¸¸ç”Ÿæ´»è¯æ±‡",
            "words": ["eat", "drink", "sleep", "walk", "run", "play", "work", "study",
                     "home", "house", "room", "kitchen", "bedroom", "bathroom",
                     "family", "father", "mother", "brother", "sister", "friend"],
        },
        "education": {
            "name": "æ•™è‚²è¯æ±‡",
            "words": ["school", "teacher", "student", "class", "lesson", "homework",
                     "exam", "test", "grade", "university", "college", "library",
                     "book", "pen", "pencil", "paper", "knowledge"],
        },
        "business": {
            "name": "å•†åŠ¡è¯æ±‡",
            "words": ["business", "company", "office", "meeting", "manager", "employee",
                     "salary", "money", "profit", "customer", "market", "sell", "buy",
                     "trade", "industry", "economy", "finance", "investment"],
        },
        "technology": {
            "name": "ç§‘æŠ€è¯æ±‡",
            "words": ["computer", "internet", "software", "hardware", "program",
                     "code", "data", "digital", "electronic", "machine", "robot",
                     "technology", "innovation", "invention", "smartphone", "laptop"],
        },
        "travel": {
            "name": "æ—…è¡Œè¯æ±‡",
            "words": ["travel", "trip", "journey", "vacation", "holiday", "hotel",
                     "flight", "plane", "train", "car", "ticket", "passport",
                     "luggage", "suitcase", "tourist", "guide", "map", "destination"],
        },
        "food": {
            "name": "é£Ÿç‰©è¯æ±‡",
            "words": ["food", "eat", "drink", "restaurant", "cafe", "menu", "order",
                     "breakfast", "lunch", "dinner", "meal", "meat", "fish", "chicken",
                     "vegetable", "fruit", "bread", "rice", "noodle", "soup"],
        },
        "health": {
            "name": "å¥åº·è¯æ±‡",
            "words": ["health", "body", "doctor", "hospital", "medicine", "nurse",
                     "patient", "disease", "sick", "pain", "headache", "fever",
                     "cold", "cure", "treat", "exercise", "sport", "fitness"],
        },
        "nature": {
            "name": "è‡ªç„¶è¯æ±‡",
            "words": ["nature", "natural", "environment", "earth", "sky", "sun", "moon",
                     "star", "cloud", "rain", "snow", "wind", "mountain", "river",
                     "ocean", "sea", "forest", "tree", "flower", "animal", "bird"],
        },
    }

    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆè¯åº“
    for category_id, category_info in categories.items():
        vocab_list = []

        for index, word in enumerate(category_info["words"], 1):
            entry = {
                "id": f"{category_id}_{index:03d}",
                "word": word,
                "phonetic": f"/{word}/",  # ç®€åŒ–éŸ³æ ‡
                "definition": f"{category_info['name']}ä¸­çš„è¯æ±‡: {word}",
                "examples": [f"Example using '{word}' in {category_info['name']} context."],
                "synonyms": [],
                "antonyms": [],
                "difficulty": 2,
                "tags": [category_id, "noun"],
                "etymology": f"Word in {category_info['name']}"
            }
            vocab_list.append(entry)

        # ä¿å­˜åˆ†ç±»è¯åº“
        filepath = f"../assets/vocabularies/{category_id}.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_list, f, ensure_ascii=False, indent=2)

        print(f"âœ… ç”Ÿæˆåˆ†ç±»è¯åº“: {category_info['name']} ({len(vocab_list)}è¯)")

def create_vocabulary_summary():
    """åˆ›å»ºè¯åº“æ€»ç»“æ–‡æ¡£"""
    print("\nğŸ“Š ç”Ÿæˆè¯åº“æ€»ç»“...")

    summary = {
        "last_updated": "2026-02-28",
        "total_vocabs": 0,
        "total_words": 0,
        "vocabularies": []
    }

    # ç»Ÿè®¡æ‰€æœ‰JSONæ–‡ä»¶
    vocab_dir = "../assets/vocabularies"
    if os.path.exists(vocab_dir):
        for file in os.listdir(vocab_dir):
            if file.endswith('.json'):
                filepath = os.path.join(vocab_dir, file)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        word_count = len(data)
                        file_size = os.path.getsize(filepath)

                        summary["vocabularies"].append({
                            "file": file,
                            "words": word_count,
                            "size_kb": round(file_size / 1024, 2)
                        })
                        summary["total_words"] += word_count
                        summary["total_vocabs"] += 1
                except Exception as e:
                    print(f"Warning: Could not process {file}: {e}")

    # ä¿å­˜æ€»ç»“
    with open(os.path.join(vocab_dir, "vocabulary_summary.json"), 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"âœ… è¯åº“æ€»ç»“å·²ç”Ÿæˆ")
    print(f"   æ€»è®¡ {summary['total_vocabs']} ä¸ªè¯åº“æ–‡ä»¶")
    print(f"   æ€»è®¡ {summary['total_words']} ä¸ªè¯æ±‡")

    return summary

def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘          ğŸ“š å®Œæ•´è¯åº“ç³»ç»Ÿç”Ÿæˆå™¨ ğŸ“š                                       â•‘")
    print("â•‘          (Complete Vocabulary System Generator)                           â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    # 1. åŠ è½½å·²æœ‰è¯æ±‡
    print("\nğŸ“– åŠ è½½å·²æœ‰è¯åº“...")
    vocab_db = load_existing_vocabs()
    print(f"âœ… å·²åŠ è½½ {len(vocab_db)} ä¸ªä¸é‡å¤è¯æ±‡")

    # 2. åˆ›å»ºåˆ†ç±»è¯åº“
    create_categorized_vocabulary()

    # 3. åˆ›å»ºè¯åº“æ€»ç»“
    summary = create_vocabulary_summary()

    # 4. æ˜¾ç¤ºè¯åº“æ¸…å•
    print(f"\n" + "=" * 70)
    print("ğŸ“‹ è¯åº“æ¸…å•")
    print("=" * 70)

    for vocab in summary["vocabularies"]:
        print(f"  â€¢ {vocab['file']:40s} {vocab['words']:4d}è¯  {vocab['size_kb']:6.1f}KB")

    print("=" * 70)
    print(f"  {'æ€»è®¡':40s} {summary['total_words']:4d}è¯  {sum(v['size_kb'] for v in summary['vocabularies']):6.1f}KB")
    print("=" * 70)

    print("\nğŸ‰ è¯åº“ç³»ç»Ÿæ„å»ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š")
    print("  â€¢ å¼€å‘æµ‹è¯•: ä½¿ç”¨ *_sample.json (100è¯)")
    print("  â€¢ æ—¥å¸¸å­¦ä¹ : ä½¿ç”¨ *_complete.json (500è¯)")
    print("  â€¢ æ·±å…¥å­¦ä¹ : ä½¿ç”¨ åˆ†ç±»è¯åº“ (æŒ‰ä¸»é¢˜)")
    print("  â€¢ å…¨é¢æŒæ¡: ç»„åˆå¤šä¸ªè¯åº“ä½¿ç”¨")

if __name__ == "__main__":
    main()
