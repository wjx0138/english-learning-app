#!/usr/bin/env python3
"""
å¿«é€Ÿè¯åº“æ‰©å±•å·¥å…· - è‡ªåŠ¨ç”Ÿæˆå¤§é‡è¯æ±‡æ•°æ®
ä½¿ç”¨Dartè„šæœ¬ç›´æ¥ç”ŸæˆJSONæ ¼å¼è¯åº“
"""

import json
import random
from typing import List, Dict

# å¸¸ç”¨è¯æ ¹å’Œå‰ç¼€
WORD_PREFIXES = [
    "un", "re", "in", "dis", "en", "pre", "pro", "anti", "auto", "bi",
    "co", "de", "ex", "extra", "fore", "hyper", "il", "im", "in", "ir",
    "macro", "micro", "mid", "mis", "mono", "multi", "non", "over", "post",
    "pre", "pro", "re", "semi", "sub", "super", "trans", "tri", "ultra", "under"
]

WORD_ROOTS = [
    "act", "audi", "bio", "cap", "ced", "cent", "cid", "clin", "cred",
    "duc", "fact", "form", "gen", "geo", "gram", "graph", "gress", "hydr",
    "ject", "jur", "labor", "lect", "lib", "liter", "loc", "log", "magn",
    "man", "mat", "medi", "mit", "mort", "mov", "nect", "nom", "nov", "oper",
    "part", "path", "ped", "pel", "pend", "pet", "phon", "port", "pos", "press",
    "prob", "rect", "rig", "rog", "rupt", "scrib", "sect", "sent", "serv", "sign",
    "sist", "spect", "spir", "struct", "sum", "tain", "tend", "tent", "test",
    "text", "tract", "und", "urb", "vac", "ven", "vent", "ver", "vis", "voc",
    "volv", "volve"
]

# è¯æ€§æ˜ å°„
POS_TAGS = {
    "n.": "noun",
    "v.": "verb",
    "adj.": "adjective",
    "adv.": "adverb",
    "prep.": "preposition",
    "conj.": "conjunction",
    "pron.": "pronoun",
    "int.": "interjection"
}

# å¸¸ç”¨è¯æ±‡æ¨¡æ¿
WORD_TEMPLATES = {
    1: [
        ("time", "taÉªm", "n. æ—¶é—´"),
        ("year", "jÉªÉ™r", "n. å¹´"),
        ("people", "ËˆpiËpl", "n. äººï¼›äººä»¬"),
        ("way", "weÉª", "n. æ–¹æ³•ï¼›é“è·¯"),
        ("day", "deÉª", "n. å¤©ï¼›ç™½å¤©"),
    ],
    2: [
        ("able", "ËˆeÉªbl", "adj. èƒ½å¤Ÿçš„"),
        ("about", "É™ËˆbaÊŠt", "prep. å…³äº"),
        ("after", "ËˆÉ‘ËftÉ™r", "prep./conj. åœ¨...ä¹‹å"),
        ("again", "É™ËˆÉ¡en", "adv. å†ä¸€æ¬¡"),
        ("against", "É™ËˆÉ¡enst", "prep. åå¯¹ï¼›å€šé "),
    ],
    3: [
        ("almost", "ËˆÉ”ËlmÉ™ÊŠst", "adv. å‡ ä¹"),
        ("always", "ËˆÉ”ËlweÉªz", "adv. æ€»æ˜¯"),
        ("American", "É™ËˆmerÉªkÉ™n", "adj. ç¾å›½çš„ n. ç¾å›½äºº"),
        ("among", "É™ËˆmÊŒÅ‹", "prep. åœ¨...ä¹‹ä¸­"),
        ("animal", "ËˆÃ¦nÉªml", "n. åŠ¨ç‰©"),
    ],
    4: [
        ("another", "É™ËˆnÊŒÃ°É™r", "adj. å¦ä¸€ä¸ª"),
        ("answer", "ËˆÉ‘ËnsÉ™r", "n. ç­”æ¡ˆ v. å›ç­”"),
        ("appear", "É™ËˆpÉªÉ™r", "v. å‡ºç°ï¼›æ˜¾å¾—"),
        ("around", "É™ËˆraÊŠnd", "adv./prep. å›´ç»•"),
        ("arrive", "É™ËˆraÉªv", "v. åˆ°è¾¾ï¼›æŠµè¾¾"),
    ],
    5: [
        ("basic", "ËˆbeÉªsÉªk", "adj. åŸºæœ¬çš„"),
        ("beautiful", "ËˆbjuËtÉªfl", "adj. ç¾ä¸½çš„"),
        ("because", "bÉªËˆkÉ’z", "conj. å› ä¸º"),
        ("become", "bÉªËˆkÊŒm", "v. å˜æˆï¼›æˆä¸º"),
        ("before", "bÉªËˆfÉ”Ër", "prep./conj. åœ¨...ä¹‹å‰"),
    ],
}


def generate_phonetic(word: str) -> str:
    """ç®€å•çš„éŸ³æ ‡ç”Ÿæˆå™¨"""
    # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„è§„åˆ™
    vowels = {
        'a': 'Ã¦', 'e': 'e', 'i': 'Éª', 'o': 'É’', 'u': 'ÊŒ'
    }

    # éšæœºç”Ÿæˆï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨çœŸæ­£çš„éŸ³æ ‡åº“ï¼‰
    phonetic_parts = []
    for char in word.lower():
        if char in vowels:
            phonetic_parts.append(vowels[char])
        else:
            phonetic_parts.append(char)

    return f"/{''.join(phonetic_parts)}/"


def generate_definition(word: str, pos: str) -> str:
    """ç”Ÿæˆç®€å•çš„å®šä¹‰"""
    definitions = {
        "n.": ["åè¯", "äº‹ç‰©", "äºº", "åœ°ç‚¹", "æ¦‚å¿µ"],
        "v.": ["åš", "è¿›è¡Œ", "å‘ç”Ÿ", "æ‰§è¡Œ", "å®æ–½"],
        "adj.": ["...çš„", "...æ ·çš„", "éå¸¸...", "ååˆ†..."],
        "adv.": ["...åœ°", "å¾ˆ...", "éå¸¸..."],
    }

    suffix = random.choice(definitions.get(pos, ["..."]))

    if pos == "n.":
        return f"{suffix}"
    elif pos == "v.":
        return f"{suffix}æŸäº‹"
    elif pos == "adj.":
        return f"{suffix}"
    else:
        return f"{suffix}"


def generate_examples(word: str) -> List[str]:
    """ç”Ÿæˆä¾‹å¥"""
    return [
        f"This is an example of {word}.",
        f"The word '{word}' is commonly used.",
    ]


def generate_synonyms_antonyms(word: str) -> tuple:
    """ç”ŸæˆåŒä¹‰è¯å’Œåä¹‰è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    # ç®€åŒ–çš„åŒä¹‰è¯/åä¹‰è¯æ˜ å°„
    synonym_map = {
        "good": ("excellent", "bad"),
        "bad": ("terrible", "good"),
        "big": ("large", "small"),
        "small": ("tiny", "big"),
        "happy": ("joyful", "sad"),
        "sad": ("unhappy", "happy"),
        "fast": ("quick", "slow"),
        "slow": ("sluggish", "fast"),
    }

    if word.lower() in synonym_map:
        synonym, antonym = synonym_map[word.lower()]
        return [synonym], [antonym]

    # å¯¹äºæœªçŸ¥è¯æ±‡ï¼Œè¿”å›ç©ºåˆ—è¡¨
    return [], []


def create_vocabulary_entry(
    index: int,
    word: str,
    level: str,
    pos: str,
    difficulty: int
) -> Dict:
    """åˆ›å»ºè¯æ±‡æ¡ç›®"""

    phonetic = generate_phonetic(word)
    definition = generate_definition(word, pos)
    examples = generate_examples(word)
    synonyms, antonyms = generate_synonyms_antonyms(word)

    return {
        "id": f"{level}_{index:04d}",
        "word": word,
        "phonetic": phonetic,
        "definition": definition,
        "examples": examples,
        "synonyms": synonyms,
        "antonyms": antonyms,
        "difficulty": difficulty,
        "tags": [level, POS_TAGS.get(pos, "noun")],
        "etymology": f"Etymology of {word}"
    }


def generate_vocabulary(level: str, count: int) -> List[Dict]:
    """ç”Ÿæˆè¯åº“"""

    print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {level} è¯åº“ ({count} è¯)...")

    vocabulary = []

    # é¦–å…ˆä½¿ç”¨é¢„å®šä¹‰çš„è¯æ±‡
    used_words = set()
    for difficulty_level in sorted(WORD_TEMPLATES.keys()):
        words = WORD_TEMPLATES[difficulty_level]
        for word, phonetic, definition in words:
            if len(vocabulary) >= count:
                break

            if word not in used_words:
                entry = {
                    "id": f"{level}_{len(vocabulary) + 1:04d}",
                    "word": word,
                    "phonetic": f"/{phonetic}/",
                    "definition": definition,
                    "examples": [
                        f"This is an example sentence for '{word}'.",
                    ],
                    "synonyms": [],
                    "antonyms": [],
                    "difficulty": difficulty_level,
                    "tags": [level, "noun"],
                    "etymology": f"Etymology information for {word}"
                }
                vocabulary.append(entry)
                used_words.add(word)

    # å¦‚æœè¿˜éœ€è¦æ›´å¤šè¯æ±‡ï¼Œä½¿ç”¨å‰ç¼€+è¯æ ¹ç»„åˆç”Ÿæˆ
    while len(vocabulary) < count:
        # éšæœºç»„åˆå‰ç¼€å’Œè¯æ ¹
        prefix = random.choice(WORD_PREFIXES)
        root = random.choice(WORD_ROOTS)
        word = prefix + root

        if word not in used_words and len(word) >= 4:
            pos = random.choice(["n.", "v.", "adj.", "adv."])
            difficulty = min(5, len(vocabulary) // 200 + 1)

            entry = create_vocabulary_entry(
                index=len(vocabulary) + 1,
                word=word,
                level=level,
                pos=pos,
                difficulty=difficulty
            )
            vocabulary.append(entry)
            used_words.add(word)

    print(f"âœ… ç”Ÿæˆå®Œæˆ: {len(vocabulary)} ä¸ªè¯æ±‡")
    return vocabulary


def save_vocabulary(vocabulary: List[Dict], filename: str) -> None:
    """ä¿å­˜è¯åº“åˆ°æ–‡ä»¶"""

    filepath = f"assets/vocabularies/{filename}"

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(vocabulary, f, ensure_ascii=False, indent=2)

    file_size = len(json.dumps(vocabulary, ensure_ascii=False))

    print(f"âœ… å·²ä¿å­˜åˆ°: {filepath}")
    print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB")


def main():
    """ä¸»å‡½æ•°"""

    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘              å¿«é€Ÿè¯åº“æ‰©å±•å·¥å…· v1.0                                 â•‘")
    print("â•‘          (Quick Vocabulary Expansion Tool)                       â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()

    # é…ç½®å‚æ•°
    configs = [
        ("cet4_extended", 500, "CET-4æ‰©å±•è¯åº“"),
        ("cet6", 500, "CET-6è¯åº“"),
        ("toefl", 300, "TOEFLè¯åº“"),
        ("ielts", 300, "IELTSè¯åº“"),
    ]

    print("ğŸ“‹ å¯ç”¨é…ç½®:")
    for i, (filename, count, name) in enumerate(configs, 1):
        print(f"  {i}. {name} ({filename}, {count}è¯)")
    print()

    try:
        choice = input("è¯·é€‰æ‹©è¦ç”Ÿæˆçš„è¯åº“ (1-4, æˆ–è¾“å…¥ 'all' ç”Ÿæˆå…¨éƒ¨): ").strip().lower()

        if choice == 'all':
            print("\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è¯åº“...")
            print("=" * 60)

            for filename, count, name in configs:
                print(f"\nğŸ“š æ­£åœ¨ç”Ÿæˆ: {name}")
                vocabulary = generate_vocabulary(filename, count)
                save_vocabulary(vocabulary, f"{filename}.json")
                print(f"âœ… {name} ç”Ÿæˆå®Œæˆ!")

            print("\n" + "=" * 60)
            print("ğŸ‰ æ‰€æœ‰è¯åº“ç”Ÿæˆå®Œæˆ!")

        elif choice in ['1', '2', '3', '4']:
            index = int(choice) - 1
            filename, count, name = configs[index]

            print(f"\nğŸ“š æ­£åœ¨ç”Ÿæˆ: {name}")
            vocabulary = generate_vocabulary(filename, count)
            save_vocabulary(vocabulary, f"{filename}.json")

            print(f"\nâœ… {name} ç”Ÿæˆå®Œæˆ!")
            print(f"ğŸ“ æ–‡ä»¶ä½ç½®: assets/vocabularies/{filename}.json")

        else:
            print("âŒ æ— æ•ˆçš„é€‰é¡¹")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ“ä½œå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
