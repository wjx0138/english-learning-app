#!/usr/bin/env python3
"""
è¯åº“ç”Ÿæˆå·¥å…· - ä»ECDICTæˆ–å…¶ä»–æ•°æ®æºç”ŸæˆFlutterè¯åº“JSONæ–‡ä»¶
ä½¿ç”¨æ–¹æ³•: python vocabulary_generator.py
"""

import json
import urllib.request
from typing import List, Dict
import os

# ECDICT æ˜Ÿçº§è¯åº“æ¥æº
ECDICT_STARS_URLS = {
    1: "https://github.com/skywind3000/ECDICT/raw/master/stardict.csv/ecdict-gui-stardict-1.csv",
    2: "https://github.com/skywind3000/ECDICT/raw/master/stardict.csv/ecdict-gui-stardict-2.csv",
    3: "https://github.com/skywind3000/ECDICT/raw/master/stardict.csv/ecdict-gui-stardict-3.csv",
}

# CET è¯è¡¨
CET_VOCABULARY = {
    "cet4": {
        "name": "CET-4",
        "description": "å¤§å­¦è‹±è¯­å››çº§è¯æ±‡",
        "difficulty_range": (1, 3),
        "word_count": 4500
    },
    "cet6": {
        "name": "CET-6",
        "description": "å¤§å­¦è‹±è¯­å…­çº§è¯æ±‡",
        "difficulty_range": (2, 4),
        "word_count": 6000
    },
    "toefl": {
        "name": "TOEFL",
        "description": "æ‰˜ç¦è¯æ±‡",
        "difficulty_range": (3, 5),
        "word_count": 8000
    },
    "ielts": {
        "name": "IELTS",
        "description": "é›…æ€è¯æ±‡",
        "difficulty_range": (3, 5),
        "word_count": 7500
    },
    "gre": {
        "name": "GRE",
        "description": "GREè¯æ±‡",
        "difficulty_range": (4, 5),
        "word_count": 12000
    }
}

class VocabularyGenerator:
    """è¯åº“ç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str = "assets/vocabularies"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def generate_word_entry(
        self,
        word: str,
        phonetic: str,
        definition: str,
        index: int,
        level: str = "cet4",
        difficulty: int = 2
    ) -> Dict:
        """ç”Ÿæˆå•ä¸ªè¯æ±‡æ¡ç›®"""

        # æ ¹æ®é¦–å­—æ¯ç”ŸæˆåŒä¹‰è¯å’Œåä¹‰è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        synonyms = self._generate_synonyms(word)
        antonyms = self._generate_antonyms(word)

        return {
            "id": f"{level}_{index:04d}",
            "word": word,
            "phonetic": f"/{phonetic}/",
            "definition": definition,
            "examples": [
                f"This is an example sentence for the word '{word}'.",
            ],
            "synonyms": synonyms[:3],  # æœ€å¤š3ä¸ªåŒä¹‰è¯
            "antonyms": antonyms[:2],  # æœ€å¤š2ä¸ªåä¹‰è¯
            "difficulty": difficulty,
            "tags": [level, self._get_pos_from_definition(definition)],
            "etymology": f"Etymology information for {word}"
        }

    def _generate_synonyms(self, word: str) -> List[str]:
        """ç”ŸæˆåŒä¹‰è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„æ˜ å°„ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä»è¯åº“æ•°æ®è·å–
        synonym_map = {
            "good": ["excellent", "fine", "great"],
            "bad": ["terrible", "poor", "awful"],
            "big": ["large", "huge", "enormous"],
            "small": ["tiny", "little", "minute"],
            "happy": ["joyful", "glad", "cheerful"],
            "sad": ["unhappy", "sorrowful", "depressed"],
            "fast": ["quick", "rapid", "swift"],
            "slow": ["sluggish", "leisurely", "unhurried"],
        }
        return synonym_map.get(word.lower(), [])

    def _generate_antonyms(self, word: str) -> List[str]:
        """ç”Ÿæˆåä¹‰è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        antonym_map = {
            "good": ["bad", "poor"],
            "bad": ["good", "excellent"],
            "big": ["small", "tiny"],
            "small": ["big", "huge"],
            "happy": ["sad", "unhappy"],
            "sad": ["happy", "joyful"],
            "fast": ["slow", "sluggish"],
            "slow": ["fast", "quick"],
        }
        return antonym_map.get(word.lower(), [])

    def _get_pos_from_definition(self, definition: str) -> str:
        """ä»å®šä¹‰ä¸­æ¨æ–­è¯æ€§"""
        if definition.startswith("v."):
            return "verb"
        elif definition.startswith("n."):
            return "noun"
        elif definition.startswith("adj."):
            return "adjective"
        elif definition.startswith("adv."):
            return "adverb"
        elif definition.startswith("prep."):
            return "preposition"
        else:
            return "noun"

    def create_sample_vocabulary(
        self,
        level: str = "cet4",
        count: int = 100
    ) -> List[Dict]:
        """åˆ›å»ºç¤ºä¾‹è¯åº“"""

        sample_words = [
            ("ability", "É™ËˆbÉªlÉ™ti", "n. èƒ½åŠ›ï¼›æœ¬é¢†"),
            ("abroad", "É™ËˆbrÉ”Ëd", "adv. åœ¨å›½å¤–ï¼›åˆ°å›½å¤–"),
            ("absence", "ËˆÃ¦bsÉ™ns", "n. ç¼ºå¸­ï¼›ä¸åœ¨"),
            ("absolute", "ËˆÃ¦bsÉ™luËt", "adj. ç»å¯¹çš„ï¼›å®Œå…¨çš„"),
            ("absorb", "É™bËˆzÉ”Ëb", "v. å¸æ”¶ï¼›åŒåŒ–"),
            ("abstract", "ËˆÃ¦bstrÃ¦kt", "adj. æŠ½è±¡çš„ n. æ‘˜è¦"),
            ("academic", "ËŒÃ¦kÉ™ËˆdemÉªk", "adj. å­¦æœ¯çš„"),
            ("accept", "É™kËˆsept", "v. æ¥å—ï¼›åŒæ„"),
            ("access", "ËˆÃ¦kses", "n. æ¥è¿‘ï¼›é€šé“"),
            ("accident", "ËˆÃ¦ksÉªdÉ™nt", "n. äº‹æ•…ï¼›æ„å¤–"),
            ("accompany", "É™ËˆkÊŒmpÉ™ni", "v. é™ªä¼´ï¼›ä¼´å¥"),
            ("accomplish", "É™ËˆkÊŒmplÉªÊƒ", "v. å®Œæˆï¼›å®ç°"),
            ("account", "É™ËˆkaÊŠnt", "n. è´¦æˆ·ï¼›è§£é‡Š"),
            ("accurate", "ËˆÃ¦kjÉ™rÉ™t", "adj. å‡†ç¡®çš„ï¼›ç²¾ç¡®çš„"),
            ("achieve", "É™ËˆtÊƒiËv", "v. å®ç°ï¼›è¾¾åˆ°"),
            ("acknowledge", "É™kËˆnÉ’lÉªdÊ’", "v. æ‰¿è®¤ï¼›è‡´è°¢"),
            ("acquire", "É™ËˆkwaÉªÉ™r", "v. è·å¾—ï¼›å–å¾—"),
            ("across", "É™ËˆkrÉ’s", "adv./prep. æ¨ªè¿‡"),
            ("action", "ËˆÃ¦kÊƒn", "n. è¡ŒåŠ¨ï¼›ä½œç”¨"),
            ("active", "ËˆÃ¦ktÉªv", "adj. æ´»è·ƒçš„ï¼›ç§¯æçš„"),
            ("activity", "Ã¦kËˆtÉªvÉ™ti", "n. æ´»åŠ¨ï¼›æ´»è·ƒ"),
            ("actual", "ËˆÃ¦ktÊƒuÉ™l", "adj. å®é™…çš„ï¼›çœŸå®çš„"),
            ("adapt", "É™ËˆdÃ¦pt", "v. é€‚åº”ï¼›æ”¹ç¼–"),
            ("addition", "É™ËˆdÉªÊƒn", "n. åŠ ï¼›å¢åŠ "),
            ("additional", "É™ËˆdÉªÊƒÉ™nl", "adj. é¢å¤–çš„ï¼›é™„åŠ çš„"),
            ("address", "É™Ëˆdres", "n. åœ°å€ v. è‡´è¾"),
            ("adequate", "ËˆÃ¦dÉªkwÉ™t", "adj. è¶³å¤Ÿçš„ï¼›é€‚å½“çš„"),
            ("adjust", "É™ËˆdÊ’ÊŒst", "v. è°ƒæ•´ï¼›é€‚åº”"),
            ("administration", "É™dËŒmÉªnÉªËˆstreÉªÊƒn", "n. ç®¡ç†ï¼›è¡Œæ”¿"),
            ("admire", "É™dËˆmaÉªÉ™r", "v. é’¦ä½©ï¼›ç¾¡æ…•"),
            ("admit", "É™dËˆmÉªt", "v. æ‰¿è®¤ï¼›å‡†è®¸è¿›å…¥"),
            ("adopt", "É™ËˆdÉ’pt", "v. æ”¶å…»ï¼›é‡‡ç”¨"),
            ("adult", "ËˆÃ¦dÊŒlt", "n. æˆå¹´äºº adj. æˆå¹´çš„"),
            ("advance", "É™dËˆvÉ‘Ëns", "v. å‰è¿› n. è¿›å±•"),
            ("advanced", "É™dËˆvÉ‘Ënst", "adj. å…ˆè¿›çš„ï¼›é«˜çº§çš„"),
            ("advantage", "É™dËˆvÉ‘ËntÉªdÊ’", "n. ä¼˜åŠ¿ï¼›åˆ©ç›Š"),
            ("adventure", "É™dËˆventÊƒÉ™r", "n. å†’é™©ï¼›å¥‡é‡"),
            ("advertise", "ËˆÃ¦dvÉ™taÉªz", "v. åšå¹¿å‘Šï¼›å®£ä¼ "),
            ("advice", "É™dËˆvaÉªs", "n. å»ºè®®ï¼›å¿ å‘Š"),
            ("advocate", "ËˆÃ¦dvÉ™kÉ™t", "v. æå€¡ n. æ‹¥æŠ¤è€…"),
            ("affair", "É™ËˆfeÉ™r", "n. äº‹æƒ…ï¼›äº‹åŠ¡"),
            ("affect", "É™Ëˆfekt", "v. å½±å“ï¼›æ„ŸåŠ¨"),
            ("affection", "É™ËˆfekÊƒn", "n. å–œçˆ±ï¼›æ„Ÿæƒ…"),
            ("afford", "É™ËˆfÉ”Ëd", "v. ä¹°å¾—èµ·ï¼›æ‰¿æ‹…"),
            ("afraid", "É™ËˆfreÉªd", "adj. å®³æ€•çš„ï¼›æ‹…å¿ƒçš„"),
            ("agency", "ËˆeÉªdÊ’É™nsi", "n. ä»£ç†å¤„ï¼›æœºæ„"),
            ("aggressive", "É™ËˆÉ¡resÉªv", "adj. ä¾µç•¥çš„ï¼›è¿›å–çš„"),
        ]

        vocabulary = []
        for i, (word, phonetic, definition) in enumerate(sample_words[:count], 1):
            entry = self.generate_word_entry(
                word=word,
                phonetic=phonetic,
                definition=definition,
                index=i,
                level=level,
                difficulty=min(3, i // 30 + 1)
            )
            vocabulary.append(entry)

        return vocabulary

    def save_vocabulary(
        self,
        vocabulary: List[Dict],
        filename: str
    ) -> None:
        """ä¿å­˜è¯åº“åˆ°JSONæ–‡ä»¶"""

        filepath = os.path.join(self.output_dir, filename)
        with open(filepath, 'w', encoding='utf-8') as File:
            json.dump(vocabulary, File, ensure_ascii=False, indent=2)

        print(f"âœ… è¯åº“å·²ä¿å­˜åˆ°: {filepath}")
        print(f"ğŸ“Š è¯æ±‡æ•°é‡: {len(vocabulary)}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {os.path.getsize(filepath) / 1024:.2f} KB")

    def generate_cet4(self, count: int = 500) -> None:
        """ç”ŸæˆCET4è¯åº“"""

        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆCET4è¯åº“ ({count}è¯)...")

        # ä»ç¤ºä¾‹è¯åº“ç”Ÿæˆ
        vocabulary = self.create_sample_vocabulary("cet4", count)

        self.save_vocabulary(vocabulary, "cet4_extended.json")

    def generate_cet6(self, count: int = 500) -> None:
        """ç”ŸæˆCET6è¯åº“"""

        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆCET6è¯åº“ ({count}è¯)...")

        # CET6åœ¨CET4åŸºç¡€ä¸Šå¢åŠ æ›´éš¾çš„è¯æ±‡
        vocabulary = []

        advanced_words = [
            ("abnormal", "Ã¦bËˆnÉ”Ëml", "adj. åå¸¸çš„ï¼›å˜æ€çš„"),
            ("abolish", "É™ËˆbÉ’lÉªÊƒ", "v. åºŸé™¤ï¼›å–æ¶ˆ"),
            ("abortion", "É™ËˆbÉ”ËÊƒn", "n. æµäº§ï¼›å •èƒ"),
            ("abridge", "É™ËˆbrÉªdÊ’", "v. åˆ èŠ‚ï¼›ç¼©çŸ­"),
            ("abroad", "É™ËˆbrÉ”Ëd", "adv. åœ¨å›½å¤–ï¼›åˆ°å›½å¤–"),
            ("abrupt", "É™ËˆbrÊŒpt", "adj. çªç„¶çš„ï¼›ç”Ÿç¡¬çš„"),
            ("absence", "ËˆÃ¦bsÉ™ns", "n. ç¼ºå¸­ï¼›ä¸åœ¨"),
            ("absolute", "ËˆÃ¦bsÉ™luËt", "adj. ç»å¯¹çš„ï¼›å®Œå…¨çš„"),
            ("absorb", "É™bËˆzÉ”Ëb", "v. å¸æ”¶ï¼›åŒåŒ–"),
            ("abstract", "ËˆÃ¦bstrÃ¦kt", "adj. æŠ½è±¡çš„ n. æ‘˜è¦"),
            ("absurd", "É™bËˆsÉœËd", "adj. è’è°¬çš„ï¼›å¯ç¬‘çš„"),
            ("abundance", "É™ËˆbÊŒndÉ™ns", "n. ä¸°å¯Œï¼›å……è£•"),
            ("abuse", "É™ËˆbjuËz", "v. æ»¥ç”¨ n. è™å¾…"),
            ("academic", "ËŒÃ¦kÉ™ËˆdemÉªk", "adj. å­¦æœ¯çš„"),
            ("academy", "É™ËˆkÃ¦dÉ™mi", "n. å­¦é™¢ï¼›å­¦ä¼š"),
            ("accelerate", "É™kËˆselÉ™reÉªt", "v. åŠ é€Ÿï¼›ä¿ƒè¿›"),
            ("accept", "É™kËˆsept", "v. æ¥å—ï¼›åŒæ„"),
            ("acceptable", "É™kËˆseptÉ™bl", "adj. å¯æ¥å—çš„"),
            ("access", "ËˆÃ¦kses", "n. æ¥è¿‘ï¼›é€šé“"),
            ("accessible", "É™kËˆsesÉ™bl", "adj. æ˜“è¾¾åˆ°çš„"),
        ]

        for i, (word, phonetic, definition) in enumerate(advanced_words[:count], 1):
            entry = self.generate_word_entry(
                word=word,
                phonetic=phonetic,
                definition=definition,
                index=i,
                level="cet6",
                difficulty=3 + (i // 50)
            )
            vocabulary.append(entry)

        self.save_vocabulary(vocabulary, "cet6.json")

    def merge_vocabularies(
        self,
        input_files: List[str],
        output_file: str
    ) -> None:
        """åˆå¹¶å¤šä¸ªè¯åº“æ–‡ä»¶"""

        print(f"ğŸ”„ å¼€å§‹åˆå¹¶è¯åº“...")

        merged_vocabulary = []
        word_ids = set()

        for input_file in input_files:
            filepath = os.path.join(self.output_dir, input_file)
            if not os.path.exists(filepath):
                print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
                continue

            with open(filepath, 'r', encoding='utf-8') as f:
                vocabulary = json.load(f)
                for word in vocabulary:
                    if word['id'] not in word_ids:
                        merged_vocabulary.append(word)
                        word_ids.add(word['id'])

            print(f"âœ… å·²åŠ è½½: {input_file} ({len(vocabulary)} è¯)")

        # æŒ‰IDæ’åº
        merged_vocabulary.sort(key=lambda x: x['id'])

        self.save_vocabulary(merged_vocabulary, output_file)

    def generate_statistics(self, filename: str) -> None:
        """ç”Ÿæˆè¯åº“ç»Ÿè®¡ä¿¡æ¯"""

        filepath = os.path.join(self.output_dir, filename)
        if not os.path.exists(filepath):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return

        with open(filepath, 'r', encoding='utf-8') as f:
            vocabulary = json.load(f)

        # ç»Ÿè®¡ä¿¡æ¯
        total_words = len(vocabulary)
        difficulty_distribution = {}
        tag_distribution = {}

        for word in vocabulary:
            # éš¾åº¦åˆ†å¸ƒ
            difficulty = word.get('difficulty', 0)
            difficulty_distribution[difficulty] = difficulty_distribution.get(difficulty, 0) + 1

            # æ ‡ç­¾åˆ†å¸ƒ
            for tag in word.get('tags', []):
                tag_distribution[tag] = tag_distribution.get(tag, 0) + 1

        print(f"\nğŸ“Š è¯åº“ç»Ÿè®¡ä¿¡æ¯ - {filename}")
        print("=" * 50)
        print(f"æ€»è¯æ±‡æ•°: {total_words}")
        print(f"\néš¾åº¦åˆ†å¸ƒ:")
        for difficulty in sorted(difficulty_distribution.keys()):
            count = difficulty_distribution[difficulty]
            percentage = (count / total_words) * 100
            print(f"  éš¾åº¦ {difficulty}: {count} è¯ ({percentage:.1f}%)")

        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        for tag, count in sorted(tag_distribution.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_words) * 100
            print(f"  {tag}: {count} è¯ ({percentage:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""

    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " " * 10 + "è¯åº“ç”Ÿæˆå·¥å…·" + " " * 36 + "â•‘")
    print("â•‘" + " " * 58 + "â•‘")
    print("â•‘" + "  ç”¨äºç”ŸæˆFlutterè‹±è¯­å­¦ä¹ åº”ç”¨çš„è¯åº“JSONæ–‡ä»¶" + " " * 22 + "â•‘")
    print("â•š" + "â•" * 58 + "â•")
    print()

    generator = VocabularyGenerator()

    # ç”Ÿæˆè¯åº“é€‰é¡¹
    print("è¯·é€‰æ‹©æ“ä½œ:")
    print("1. ç”ŸæˆCET4è¯åº“ (500è¯)")
    print("2. ç”ŸæˆCET6è¯åº“ (500è¯)")
    print("3. åˆå¹¶è¯åº“æ–‡ä»¶")
    print("4. æŸ¥çœ‹è¯åº“ç»Ÿè®¡")
    print("5. æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è¯åº“")
    print()

    try:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (1-5): ").strip()

        if choice == "1":
            generator.generate_cet4(count=500)

        elif choice == "2":
            generator.generate_cet6(count=500)

        elif choice == "3":
            print("å¯ç”¨çš„è¯åº“æ–‡ä»¶:")
            for file in os.listdir(generator.output_dir):
                if file.endswith('.json'):
                    print(f"  - {file}")

            input_files = input("\nè¯·è¾“å…¥è¦åˆå¹¶çš„æ–‡ä»¶å (ç”¨é€—å·åˆ†éš”): ").strip()
            file_list = [f.strip() for f in input_files.split(',')]
            output_file = input("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å: ").strip()

            generator.merge_vocabularies(file_list, output_file)

        elif choice == "4":
            print("å¯ç”¨çš„è¯åº“æ–‡ä»¶:")
            for file in os.listdir(generator.output_dir):
                if file.endswith('.json'):
                    print(f"  - {file}")

            filename = input("\nè¯·è¾“å…¥æ–‡ä»¶å: ").strip()
            generator.generate_statistics(filename)

        elif choice == "5":
            print("\nğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆæ‰€æœ‰è¯åº“...")
            print("-" * 50)
            generator.generate_cet4(count=500)
            print()
            generator.generate_cet6(count=500)
            print()
            print("âœ… æ‰€æœ‰è¯åº“ç”Ÿæˆå®Œæˆï¼")

        else:
            print("âŒ æ— æ•ˆçš„é€‰é¡¹")

    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æ“ä½œå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
